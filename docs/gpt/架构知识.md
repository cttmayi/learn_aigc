
# 架构知识

## RNN/LSTM/GRU时代

自然语言处理（**NLP**）的研究可以追溯到 20 世纪 40 年代。早期NLP技术是基于规则的，即基于特定的规则使用程序/代码和知识图谱进行固定模式的对话，所有的应答都是固定模式和已有知识库的。基于规则的NLP不容易出现幻觉，但是应答能力仅限于与特定领域相关的内容，且不容易形成逻辑推理。在机器学习和深度学习诞生后，NLP技术逐渐进入基于模型的时代。

![基于规则的NLP V.S. 基于机器学习的NLP V.S. 基于深度学习的NLP](/pics/NLP.jpeg)

在进入深度学习模型阶段后，早期的NLP文本生成是通过传统循环神经网络 (**RNN**)、长短时记忆神经网络 (**LSTM**)或门控循环单元 (**GRU**)实现的。这三类经典模型都是基于循环结构实现的。

传统循环神经网络 (**RNN**)包含串行数据点之间的依赖关系（前后或时间关系），并将记忆的概念引入神经网络。通过训练长程 RNN单元 记住基于上下文的概念，从而学习到这些可重复的模式。

但循环神经网络存在短时记忆问题。对于很长的语言/文本序列，将信息从较早的单元传递到较晚的单元容易出现失真。特别是在处理大段长文本时，RNN甚至有可能在一开始的传输中就遗漏或丢失重要信息。另外，在反向传播训练过程中，RNN容易遇到梯度消失这类训练难题。

之后提出的LSTM 和 GRU 是解决RNN短时记忆问题的优化模型。**LSTM** 和 **GRU**内部通过门控的机制，进行信息要素的调节。门控可以调节并保留序列中的重要信息，丢弃非关键信息，将有价值的数据信息传递到长序列链中的后级进行计算。**LSTM** 和 **GRU** 已经广泛用于各类云边端的语音识别、语音合成和文本生成。

这三类模型能够较好的进行**模式识别**，在输出单个单词或短语方面表现良好，但无法生成高精度的多轮对话，更无法实现逻辑推理能力。

![RNN-LSTM-GRU](/pics/RNN-LSTM-GRU.png)

## GPT-1与无监督训练

2018年6月，OpenAI 发表了GPT-1，以预训练生成式变换器为名的GPT家族首次登上历史舞台。

在GPT-1这项工作之前，绝大多数最先进的 NLP 模型都是使用监督学习，专门针对特定垂直领域任务（如情感分类等）进行训练。但已有的监督模型有两个主要限制：

* 一方面需要大量带有标注的训练数据来学习不容易获得的特定任务，

* 另一方面无法将现有模型泛化到训练数据集以外的任务。

也就是说，如果在真实生活场景使用有监督模型，训练数据集的构建任务将异常庞大和琐碎，数据集的标注成本会异常的高。为了解决这些问题，OpenAI提出了GPT-1。

GPT-1的核心是[Transformer](/docs/gpt/transformer.md)，通过两个途径针对性的解决上述的两个问题：

* 通过无监督训练，解决需要大量高质量标注数据的问题，降低数据成本。

* 通过大量不同类型混杂语料进行预训练，解决训练任务的泛化问题。



在GPT-1诞生的时代，AR（Auto-regressive，自回归）与AE（Auto-encoding，自编码）是无监督学习中的两大路径。AR类语言模型基于大量语料统计，基于单向或双向上下文进行文本生成，对数据的前后相对关系敏感，适合文本生成类任务。AE则通过被掩盖的输入重建原始数据（例如BERT），弥补了单向或双向信息的损失，但却由于信息的掩盖可能导致训练与实际推理的偏差。换句话说，BERT这类基于编码器的模型训练难度可能是高于同尺寸GPT类的。

GPT-1 是一个典型的自回归语言模型，只具有解码器模块。GPT-1模型使用 768大小词嵌入。使用 12 层模型，每个自注意力层有 12 个注意力头，并使用GELU 作为激活函数。

GPT-1模型训练使用了BooksCorpus数据集。训练主要包含两个阶段：第一个阶段，先利用大量无标注的语料（节约标注成本）预训练一个语言模型。在BooksCorpus 数据集上训练后，GPT-1能够学习到大范围的文本/词汇的隐含关系，并在包含连续文本和长文的多样化语料库中获取大量知识。

接着，在第二个阶段对预训练好的语言模型进行精调，将其迁移到各种有监督的NLP任务。也就是后文提到的“预训练+精调”模式。

OpenAI团队也在GPT-1的研究中评估了模型层数对无监督预训练和有监督精调的影响。

层数的影响，从无监督的预训练到有监督的目标任务。



## GPT-2与Zero-shot Learning

2019年，OpenAI 发表了另一篇关于他们最新模型 GPT-2 的论文（ Language Models are Unsupervised Multitask Learners ）。该模型开源并在很多NLP任务中使用。相对GPT-1，GPT-2是泛化能力更强的词向量模型，尽管并没有过多的结构创新，但是训练数据集（WebText，来自于Reddit上高赞的文章）和**模型参数量更大**。目前很多开源的GPT类模型是基于GPT-2进行的结构修改或优化。

GPT-2 有 15 亿个（1.5B）参数。是 GPT-1（117M 参数）的 10 倍以上。GPT-2与 GPT-1 的主要区别包括：

* GPT-2 有 48 层，使用 1600 维向量进行词嵌入，使用了 50,257 维标记的更大词汇量。使用1024大小的上下文窗口。

* GPT-2的层归一化被移动到每个子块的输入，并在最终的自注意力块之后添加了一个额外的层进行归一化。

* GPT-2在初始化时，残差层的权重按 1/√N 缩放，其中 N 是残差层的数量。

GPT-2的研发团队训练了 117M（与 GPT-1 相同）、345M、762M 和 1.5B参数的四种语言模型，并比较了不同大小的模型精度。在同一数据集上，模型的性能随着参数数量的增加而提升。这一现象事实上导致了后来的语言模型的尺寸的迅速增加。

GPT 2 的一个关键能力是零样本学习（**Zero-shot Learning**）。Zero-shot是零样本任务迁移的一种特殊情况，即在没有提供示例情况下，模型根据给定的指令理解特定任务。

GPT-2使用的 WebText 的数据集包含来自超过 800 万份文档的 40GB 文本数据。比 GPT-1 模型的 BookCorpus 数据集更加庞大。

## GPT-3与上下文学习

2020年6月，OpenAI 发表了另一篇关于GPT-3 模型的论文（Language Models are Few-Shot Learners）。该模型的参数是 GPT-2 的100 倍以上（175B），并且在更大的文本数据集（低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia）上进行训练，从而获得更好的模型性能。

GPT-3实际上是由多个版本组成的**第3代家族**，具有不同数量的参数和所需的计算资源。包括专门用于代码编程的Code系列。GPT-3的后继知名版本包括InstructGPT和ChatGPT。

由于使用了大量参数和广泛的数据集， GPT-3在零样本和少样本设置中的下游 NLP 任务上表现良好，其编写的文章与人类撰写的在很多时候难以区分，继续延续了GPT-2越大越好的理念。GPT-3还可以执行从未明确训练过的即时任务，例如求和数字、编写 SQL 查询和代码、解读句子中的单词。

GPT-3 有 96 层Transformer层，每层有 96 个注意力头。GPT-3 的词嵌入大小从 GPT-2 的 1600 增加到 12888。上下文窗口大小也从 GPT-2 的 1024 个增加到 GPT-3 的 2048个。并且使用了局部带状稀疏注意力模式。

GPT-3一个突出的特点是上下文学习（In-Context Learning）。在学习预测给定上下文词汇的下一个词汇的同时，GPT-3也学习或模仿数据中的模式，通过对应的关键信息匹配和模式模仿来输出对应情境下的回答。在问答中出现少量提示信息时，语言模型会将示例的模式与其训练时从类似数据中学到的知识或模式进行匹配，并使用知识或模式来应答。上下文学习能力，随着模型参数数量的增加而增强。这一显著能力同样体现在了ChatGPT和GPT-4中。

GPT-3 在五个不同语料库（Common Crawl、WebText2、Books1、Books2 和 Wikipedia。）的混合训练数据集上进行训练，每个语料库都分配不同的权重。相对高质量的数据集权重更高，且使用频次超过1次。 从这时我们可以注意到，对于大语言模型，训练的效果很大程度要要依靠于高质量的语料。

## GPT-3.5/ChatGPT与指令微调

2022 年 3 月 15 日，OpenAI 发布了名为“text-davinci-003”的新版模型，该模型被描述为比以前版本的 GPT 更强大更具有逻辑能力。目前有若干个属于GPT-3.5 系列的模型分支，其中code-davinci针对代码完成任务进行了优化。

**ChatGPT** 是基于GPT-3.5（Generative Pre-trained Transformer 3.5）架构开发的对话AI模型，是InstructGPT 的兄弟模型，也是GPT-3.5系列的最高规格。ChatGPT很可能是OpenAI 在GPT-4 正式推出之前的演练，或用于**收集大量对话数据**。

OpenAI使用 **RLHF**（Reinforcement Learning from Human Feedbac，人类反馈强化学习） 技术对 ChatGPT 进行了训练，且加入了更多**人工监督**进行微调。

ChatGPT 具有以下特征：

* 可以主动承认自身错误。若用户指出其错误，模型会听取意见并优化答案。

* ChatGPT 可以质疑不正确的问题。例如被询问 “哥伦布 2015 年来到美国的情景” 的问题时，机器人会说明哥伦布不属于这一时代并调整输出结果。

* ChatGPT 可以承认自身的无知，承认对专业技术的不了解。

* 支持连续多轮对话。

与大家在生活中用到的各类智能音箱和“**人工智障**“不同，ChatGPT在对话过程中会记忆先前使用者的对话讯息，即上下文理解，以回答某些假设性的问题。ChatGPT可实现**连续对话**，极大的提升了**对话交互模式**下的用户体验。
