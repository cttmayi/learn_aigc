# Few-shot

虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型生成响应。

让我们通过Brown等人2020年(opens in a new tab)提出的一个例子来演示少样本提示。在这个例子中，任务是在句子中正确使用一个新词。

* 提示

`“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：`
`我们在非洲旅行时看到了这些非常可爱的whatpus。`
`“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：`

* 输出

`当我们赢得比赛时，我们都开始庆祝跳跃。`
`我们可以观察到，模型通过提供一个示例（即1-shot）已经学会了如何执行任务。对于更困难的任务，我们可以尝试增加演示（例如3-shot、5-shot、10-shot等）。`

One-shot Learning是Few-shot Learning的一种特殊情况


论文链接：[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

